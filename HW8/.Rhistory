old.value.function = old.value.function,
new.value.function = new.value.function,
old.point=old.point, new.point=new.point
)
if(exists("iters")) {
iters <- rbind(iters, data.output)
} else {
iters = data.output
}
iterations = iterations + 1
old.value.function = new.value.function
if(iterations >= max_iter) break
}
return(list(converged = converged,
num_iterations = iterations,
old.value.function = old.value.function,
new.value.function = new.value.function,
coefs = new.point,
iters = iters))
}
for(step_size in mystepsize)
{
for(tol in mytol)
{
results = gradientDesc(objective.function, mystartpoint, step_size,
tol, 30000)
print(paste0("The results for tolerance = ", tol, " , and step size = ",
step_size, " is:"))
print(paste0("Minimum value is: ", results$new.value.function))
print(paste0("Coordinate coefficients are: x_1 = ", results$coefs[1],
" , and x_2 = ", results$coefs[2]))
print(paste0("Number of iteration is: ", results$num_iterations))
}
}
# gradient descent for 2D differentiable functions
#define objective function to be optimized
objective.function = function(x)
{
4*x[1]^2 + 2*x[2]^2 + 4*x[1]*x[2] + 5*x[1] + 2*x[2]
}
# define the derivative for a 2D objective function
derivative = function(x)
{
c(8*x[1] + 4*x[2] + 5, 4*x[2] + 4*x[1] + 2)
}
# here we define the step size
mystepsize = c(0.01, 0.05, 0.1)
# here we define the tolerance of the convergence criterion
mytol = c(0.0001, 0.000001, 0.00000001)
# starting point
mystartpoint = c(1,2)
gradientDesc = function(obj.function, startpoint,
stepsize, conv_threshold, max_iter)
{
old.point = startpoint
gradient = derivative(old.point)
new.point = c(old.point[1] - stepsize*gradient[1],
old.point[2] - stepsize*gradient[2])
old.value.function = obj.function(new.point)
converged = F
iterations = 0
while(converged == F) {
## Implement the gradient descent algorithm
old.point = new.point
gradient = derivative(old.point)
new.point = c(old.point[1] - stepsize*gradient[1],
old.point[2] - stepsize*gradient[2])
new.value.function = obj.function(new.point)
if( abs(old.value.function - new.value.function) <= conv_threshold) {
converged = T
}
data.output = data.frame(iteration = iterations,
old.value.function = old.value.function,
new.value.function = new.value.function,
old.point=old.point, new.point=new.point
)
if(exists("iters")) {
iters <- rbind(iters, data.output)
} else {
iters = data.output
}
iterations = iterations + 1
old.value.function = new.value.function
if(iterations >= max_iter) break
}
return(list(converged = converged,
num_iterations = iterations,
old.value.function = old.value.function,
new.value.function = new.value.function,
coefs = new.point,
iters = iters))
}
for(step_size in mystepsize)
{
for(tol in mytol)
{
results = gradientDesc(objective.function, mystartpoint, step_size,
tol, 30000)
print(paste0("The results for tolerance = ", tol, " , and step size = ",
step_size, " is:"))
print(paste0("Minimum value is: ", results$new.value.function))
print(paste0("Coordinate coefficients are: x_1 = ", results$coefs[1],
" , and x_2 = ", results$coefs[2]))
print(paste0("Number of iteration is: ", results$num_iterations))
print()
}
}
# gradient descent for 2D differentiable functions
#define objective function to be optimized
objective.function = function(x)
{
4*x[1]^2 + 2*x[2]^2 + 4*x[1]*x[2] + 5*x[1] + 2*x[2]
}
# define the derivative for a 2D objective function
derivative = function(x)
{
c(8*x[1] + 4*x[2] + 5, 4*x[2] + 4*x[1] + 2)
}
# here we define the step size
mystepsize = c(0.01, 0.05, 0.1)
# here we define the tolerance of the convergence criterion
mytol = c(0.0001, 0.000001, 0.00000001)
# starting point
mystartpoint = c(1,2)
gradientDesc = function(obj.function, startpoint,
stepsize, conv_threshold, max_iter)
{
old.point = startpoint
gradient = derivative(old.point)
new.point = c(old.point[1] - stepsize*gradient[1],
old.point[2] - stepsize*gradient[2])
old.value.function = obj.function(new.point)
converged = F
iterations = 0
while(converged == F) {
## Implement the gradient descent algorithm
old.point = new.point
gradient = derivative(old.point)
new.point = c(old.point[1] - stepsize*gradient[1],
old.point[2] - stepsize*gradient[2])
new.value.function = obj.function(new.point)
if( abs(old.value.function - new.value.function) <= conv_threshold) {
converged = T
}
data.output = data.frame(iteration = iterations,
old.value.function = old.value.function,
new.value.function = new.value.function,
old.point=old.point, new.point=new.point
)
if(exists("iters")) {
iters <- rbind(iters, data.output)
} else {
iters = data.output
}
iterations = iterations + 1
old.value.function = new.value.function
if(iterations >= max_iter) break
}
return(list(converged = converged,
num_iterations = iterations,
old.value.function = old.value.function,
new.value.function = new.value.function,
coefs = new.point,
iters = iters))
}
for(step_size in mystepsize)
{
for(tol in mytol)
{
results = gradientDesc(objective.function, mystartpoint, step_size,
tol, 30000)
print(paste0("The results for tolerance = ", tol, " , and step size = ",
step_size, " is:"))
print(paste0("Minimum value is: ", results$new.value.function))
print(paste0("Coordinate coefficients are: x_1 = ", results$coefs[1],
" , and x_2 = ", results$coefs[2]))
print(paste0("Number of iteration is: ", results$num_iterations))
print("")
}
}
# gradient descent for 2D differentiable functions
#define objective function to be optimized
objective.function = function(x)
{
4*x[1]^2 + 2*x[2]^2 + 4*x[1]*x[2] + 5*x[1] + 2*x[2]
}
# define the derivative for a 2D objective function
derivative = function(x)
{
c(8*x[1] + 4*x[2] + 5, 4*x[2] + 4*x[1] + 2)
}
# here we define the step size
mystepsize = c(0.01, 0.05, 0.1)
# here we define the tolerance of the convergence criterion
mytol = c(0.0001, 0.000001, 0.00000001)
# starting point
mystartpoint = c(1,2)
gradientDesc = function(obj.function, startpoint,
stepsize, conv_threshold, max_iter)
{
old.point = startpoint
gradient = derivative(old.point)
new.point = c(old.point[1] - stepsize*gradient[1],
old.point[2] - stepsize*gradient[2])
old.value.function = obj.function(new.point)
converged = F
iterations = 0
while(converged == F) {
## Implement the gradient descent algorithm
old.point = new.point
gradient = derivative(old.point)
new.point = c(old.point[1] - stepsize*gradient[1],
old.point[2] - stepsize*gradient[2])
new.value.function = obj.function(new.point)
if( abs(old.value.function - new.value.function) <= conv_threshold) {
converged = T
}
data.output = data.frame(iteration = iterations,
old.value.function = old.value.function,
new.value.function = new.value.function,
old.point=old.point, new.point=new.point
)
if(exists("iters")) {
iters <- rbind(iters, data.output)
} else {
iters = data.output
}
iterations = iterations + 1
old.value.function = new.value.function
if(iterations >= max_iter) break
}
return(list(converged = converged,
num_iterations = iterations,
old.value.function = old.value.function,
new.value.function = new.value.function,
coefs = new.point,
iters = iters))
}
for(step_size in mystepsize)
{
for(tol in mytol)
{
results = gradientDesc(objective.function, mystartpoint, step_size,
tol, 30000)
cat(paste0("The results for tolerance = ", tol, " , and step size = ",
step_size, " is:\n"))
cat(paste0("Minimum value is: ", results$new.value.function, "\n"))
cat(paste0("Coordinate coefficients are: x_1 = ", results$coefs[1],
" , and x_2 = ", results$coefs[2], "\n"))
cat(paste0("Number of iteration is: ", results$num_iterations, "\n"))
cat("\n\n")
}
}
# gradient descent for 2D differentiable functions
#define objective function to be optimized
objective.function = function(x)
{
4*x[1]^2 + 2*x[2]^2 + 4*x[1]*x[2] + 5*x[1] + 2*x[2]
}
# define the derivative for a 2D objective function
derivative = function(x)
{
c(8*x[1] + 4*x[2] + 5, 4*x[2] + 4*x[1] + 2)
}
# here we define the step size
mystepsize = c(0.01, 0.05, 0.1)
# here we define the tolerance of the convergence criterion
mytol = c(0.0001, 0.000001, 0.00000001)
# starting point
mystartpoint = c(1,2)
gradientDesc = function(obj.function, startpoint,
stepsize, conv_threshold, max_iter)
{
old.point = startpoint
gradient = derivative(old.point)
new.point = c(old.point[1] - stepsize*gradient[1],
old.point[2] - stepsize*gradient[2])
old.value.function = obj.function(new.point)
converged = F
iterations = 0
while(converged == F) {
## Implement the gradient descent algorithm
old.point = new.point
gradient = derivative(old.point)
new.point = c(old.point[1] - stepsize*gradient[1],
old.point[2] - stepsize*gradient[2])
new.value.function = obj.function(new.point)
if( abs(old.value.function - new.value.function) <= conv_threshold) {
converged = T
}
data.output = data.frame(iteration = iterations,
old.value.function = old.value.function,
new.value.function = new.value.function,
old.point=old.point, new.point=new.point
)
if(exists("iters")) {
iters <- rbind(iters, data.output)
} else {
iters = data.output
}
iterations = iterations + 1
old.value.function = new.value.function
if(iterations >= max_iter) break
}
return(list(converged = converged,
num_iterations = iterations,
old.value.function = old.value.function,
new.value.function = new.value.function,
coefs = new.point,
iters = iters))
}
for(tol in mytol)
{
for(step_size in mystepsize)
{
results = gradientDesc(objective.function, mystartpoint, step_size,
tol, 30000)
cat(paste0("The results for tolerance = ", tol, " , and step size = ",
step_size, " is:\n"))
cat(paste0("Minimum value is: ", results$new.value.function, "\n"))
cat(paste0("Coordinate coefficients are: x_1 = ", results$coefs[1],
" , and x_2 = ", results$coefs[2], "\n"))
cat(paste0("Number of iteration is: ", results$num_iterations, "\n"))
cat("\n\n")
}
}
library(ggplot2)
# gradient descent for 2D differentiable functions
#define objective function to be optimized
objective.function = function(x)
{
4*x[1]^2 + 2*x[2]^2 + 4*x[1]*x[2] + 5*x[1] + 2*x[2]
}
# define the derivative for a 2D objective function
derivative = function(x)
{
c(8*x[1] + 4*x[2] + 5, 4*x[2] + 4*x[1] + 2)
}
# here we define the step size
mystepsize = c(0.01, 0.05, 0.1)
# here we define the tolerance of the convergence criterion
mytol = c(0.0001, 0.000001, 0.00000001)
# starting point
mystartpoint = c(1,2)
gradientDesc = function(obj.function, startpoint,
stepsize, conv_threshold, max_iter)
{
old.point = startpoint
gradient = derivative(old.point)
new.point = c(old.point[1] - stepsize*gradient[1],
old.point[2] - stepsize*gradient[2])
old.value.function = obj.function(new.point)
converged = F
iterations = 0
while(converged == F) {
## Implement the gradient descent algorithm
old.point = new.point
gradient = derivative(old.point)
new.point = c(old.point[1] - stepsize*gradient[1],
old.point[2] - stepsize*gradient[2])
new.value.function = obj.function(new.point)
if( abs(old.value.function - new.value.function) <= conv_threshold) {
converged = T
}
data.output = data.frame(iteration = iterations,
old.value.function = old.value.function,
new.value.function = new.value.function,
old.point=old.point, new.point=new.point
)
if(exists("iters")) {
iters <- rbind(iters, data.output)
} else {
iters = data.output
}
iterations = iterations + 1
old.value.function = new.value.function
if(iterations >= max_iter) break
}
return(list(converged = converged,
num_iterations = iterations,
old.value.function = old.value.function,
new.value.function = new.value.function,
coefs = new.point,
iters = iters))
}
for(tol in mytol)
{
for(step_size in mystepsize)
{
results = gradientDesc(objective.function, mystartpoint, step_size,
tol, 30000)
ggplot(data = results$iters, mapping = aes(x = iteration, y = new.value.function))+
geom_line()
cat(paste0("The results for tolerance = ", tol, " , and step size = ",
step_size, " is:\n"))
cat(paste0("Minimum value is: ", results$new.value.function, "\n"))
cat(paste0("Coordinate coefficients are: x_1 = ", results$coefs[1],
" , and x_2 = ", results$coefs[2], "\n"))
cat(paste0("Number of iteration is: ", results$num_iterations, "\n"))
cat("\n\n")
}
}
library(ggplot2)
# gradient descent for 2D differentiable functions
#define objective function to be optimized
objective.function = function(x)
{
4*x[1]^2 + 2*x[2]^2 + 4*x[1]*x[2] + 5*x[1] + 2*x[2]
}
# define the derivative for a 2D objective function
derivative = function(x)
{
c(8*x[1] + 4*x[2] + 5, 4*x[2] + 4*x[1] + 2)
}
# here we define the step size
mystepsize = c(0.01, 0.05, 0.1)
# here we define the tolerance of the convergence criterion
mytol = c(0.0001, 0.000001, 0.00000001)
# starting point
mystartpoint = c(1,2)
gradientDesc = function(obj.function, startpoint,
stepsize, conv_threshold, max_iter)
{
old.point = startpoint
gradient = derivative(old.point)
new.point = c(old.point[1] - stepsize*gradient[1],
old.point[2] - stepsize*gradient[2])
old.value.function = obj.function(new.point)
converged = F
iterations = 0
while(converged == F) {
## Implement the gradient descent algorithm
old.point = new.point
gradient = derivative(old.point)
new.point = c(old.point[1] - stepsize*gradient[1],
old.point[2] - stepsize*gradient[2])
new.value.function = obj.function(new.point)
if( abs(old.value.function - new.value.function) <= conv_threshold) {
converged = T
}
data.output = data.frame(iteration = iterations,
old.value.function = old.value.function,
new.value.function = new.value.function,
old.point=old.point, new.point=new.point
)
if(exists("iters")) {
iters <- rbind(iters, data.output)
} else {
iters = data.output
}
iterations = iterations + 1
old.value.function = new.value.function
if(iterations >= max_iter) break
}
return(list(converged = converged,
num_iterations = iterations,
old.value.function = old.value.function,
new.value.function = new.value.function,
coefs = new.point,
iters = iters))
}
for(tol in mytol)
{
for(step_size in mystepsize)
{
results = gradientDesc(objective.function, mystartpoint, step_size,
tol, 30000)
p <- ggplot(data = results$iters, mapping = aes(x = iteration, y = new.value.function))+
geom_line()
cat(paste0("The results for tolerance = ", tol, " , and step size = ",
step_size, " is:\n"))
cat(paste0("Minimum value is: ", results$new.value.function, "\n"))
cat(paste0("Coordinate coefficients are: x_1 = ", results$coefs[1],
" , and x_2 = ", results$coefs[2], "\n"))
cat(paste0("Number of iteration is: ", results$num_iterations, "\n"))
print(p)
cat("\n\n")
}
}
data("marketing", package = "datarium")
install.packages("datarium")
remove.packages("datarium")
install.packages("datarium")
data("marketing", package = "datarium")
y=marketing$sales # create response variable
x1=marketing$youtube # create x1
x2=marketing$facebook # create x2
x3=marketing$newspaper # create x3
x0=rep(1,200) # create a vector of ones
X=rbind(x0,x1,x2,x3) # create the design matrix
View(X)
beta=ginv(t(X)%*%X)%*%(t(X)%*%t(y))
library(MASS)
beta=ginv(t(X)%*%X)%*%(t(X)%*%t(y))
y=marketing$sales # create response variable
x1=marketing$youtube # create x1
x2=marketing$facebook # create x2
x3=marketing$newspaper # create x3
x0=rep(1,200) # create a vector of ones
X=cbind(x0,x1,x2,x3) # create the design matrix
beta=ginv(t(X)%*%X)%*%(t(X)%*%t(y))
t(X)%*%X
X
beta=ginv(t(X)%*%X)%*%(t(X)%*%(y))
print(beta)
pwd
setwd("/Users/owen/Documents/UCLA/CS/CS 161/HW/HW8")
read.table("sambot.dat")
data <- read.table("sambot.dat")
View(data)
data <- read.table("sambot.dat", sep = ',')
View(data)
